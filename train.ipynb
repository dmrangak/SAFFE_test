{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#last update march 6 8pm\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "#%matplotlib inline\n",
    "#from IPython.display import set_matplotlib_formats\n",
    "#set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.types import _size\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "## Audio\n",
    "from transformers import AutoProcessor, ASTModel\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, ASTModel\n",
    "\n",
    "####image\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModel , CLIPVisionModelWithProjection\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "from transformers import AutoTokenizer, CLIPTextModel,AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model import Transformer\n",
    "#from dataset import BilingualDataset, causal_mask\n",
    "from config import get_config, get_weights_file_path, latest_weights_file_path\n",
    "\n",
    "#import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import Audio\n",
    "from datasets import Dataset, Image\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/maithri/.cache/huggingface/datasets/detection-datasets___parquet/detection-datasets--coco-64ef6d5414f6b8df/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3a588988e3400e98d8f0c112cfad0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maithri/.local/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"detection-datasets/coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_test=load_dataset('detection-datasets/coco', split='train').train_test_split(train_size=117200)\n",
    "#dataset_test=load_dataset('detection-datasets/coco', split='train').train_test_split(train_size=10000)\n",
    "#dataset_val=load_dataset('detection-datasets/coco', split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "# config the database  \n",
    "def get_ds():\n",
    "    #dataset_test=load_dataset('detection-datasets/coco', split='train').train_test_split(train_size=117200)\n",
    "    dataset_test=load_dataset('detection-datasets/coco', split='train').train_test_split(train_size=50000)\n",
    "    dataset_val=load_dataset('detection-datasets/coco', split='val') \n",
    "    \n",
    "   # train_ds_size=int(0.9*len(dataset))\n",
    "    #val_ds_size= len(dataset) - train_ds_size\n",
    "    #train_dataset_both ,val_dataset_both = random_split(dataset ,[train_ds_size, val_ds_size])\n",
    "    #batch_size=config['batch_size'] \n",
    "    train_dataloader = 1\n",
    "    val_dataloader = 1\n",
    "    \n",
    "    return dataset_test['train'] ,dataset_val ,train_dataloader,val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cocolabel():\n",
    "    \n",
    "    cococlasslabel=[\"person\",\n",
    "        \"bicycle\",\n",
    "        \"car\",\n",
    "        \"motorcycle\",\n",
    "        \"airplane\",\n",
    "        \"bus\",\n",
    "        \"train\",\n",
    "        \"truck\",\n",
    "        \"boat\",\n",
    "        \"traffic light\",\n",
    "        \"fire hydrant\",\n",
    "        \"stop sign\",\n",
    "        \"parking meter\",\n",
    "        \"bench\",\n",
    "        \"bird\",\n",
    "        \"cat\",\n",
    "        \"dog\",\n",
    "        \"horse\",\n",
    "        \"sheep\",\n",
    "        \"cow\",\n",
    "        \"elephant\",\n",
    "        \"bear\",\n",
    "        \"zebra\",\n",
    "        \"giraffe\",\n",
    "        \"backpack\",\n",
    "        \"umbrella\",\n",
    "        \"handbag\",\n",
    "        \"tie\",\n",
    "        \"suitcase\",\n",
    "        \"frisbee\",\n",
    "        \"skis\",\n",
    "        \"snowboard\",\n",
    "        \"sports ball\",\n",
    "        \"kite\",\n",
    "        \"baseball bat\",\n",
    "        \"baseball glove\",\n",
    "        \"skateboard\",\n",
    "        \"surfboard\",\n",
    "        \"tennis racket\",\n",
    "        \"bottle\",\n",
    "        \"wine glass\",\n",
    "        \"cup\",\n",
    "        \"fork\",\n",
    "        \"knife\",\n",
    "        \"spoon\",\n",
    "        \"bowl\",\n",
    "        \"banana\",\n",
    "        \"apple\",\n",
    "        \"sandwich\",\n",
    "        \"orange\",\n",
    "        \"broccoli\",\n",
    "        \"carrot\",\n",
    "        \"hot dog\",\n",
    "        \"pizza\",\n",
    "        \"donut\",\n",
    "        \"cake\",\n",
    "        \"chair\",\n",
    "        \"couch\",\n",
    "        \"potted plant\",\n",
    "        \"bed\",\n",
    "        \"dining table\",\n",
    "        \"toilet\",\n",
    "        \"tv\",\n",
    "        \"laptop\",\n",
    "        \"mouse\",\n",
    "        \"remote\",\n",
    "        \"keyboard\",\n",
    "        \"cell phone\",\n",
    "        \"microwave\",\n",
    "        \"oven\",\n",
    "        \"toaster\",\n",
    "        \"sink\",\n",
    "        \"refrigerator\",\n",
    "        \"book\",\n",
    "        \"clock\",\n",
    "        \"vase\",\n",
    "        \"scissors\",\n",
    "        \"teddy bear\",\n",
    "        \"hair drier\",\n",
    "        \"toothbrush\"]\n",
    "   \n",
    "    return cococlasslabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "def get_model(): \n",
    "    model = Transformer(config[\"d_model\"], config[\"ffn_hidden\"], config[\"num_heads\"], config[\"drop_prob\"], config[\"num_layers\"], config[\"kn_vocab_size\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_model():\n",
    "\n",
    "    image_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\",output_hidden_states =True,output_attentions=True)\n",
    "    image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    return image_model,image_processor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_model():\n",
    "\n",
    "    processor_audio = AutoProcessor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "    model_audio = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", output_hidden_states =True,output_attentions=True)\n",
    "    \n",
    "\n",
    "    return model_audio,processor_audio  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_model():\n",
    "    text_model = AutoModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\",output_hidden_states =True,output_attentions=True)\n",
    "    text_Tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    return text_model ,text_Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_layer():\n",
    "    \n",
    "    image_layerno_1=10\n",
    "    image_layerno_2=9\n",
    "    print(\"image layer number\",image_layerno_1,image_layerno_2)\n",
    "    return image_layerno_1 ,image_layerno_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_validation(model, validation_ds, device, print_msg, global_step, writer, num_examples=10):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    image_model,image_processor =get_image_model()\n",
    "    model_audio,processor_audio =get_audio_model()\n",
    "    text_model,text_Tokenizer =get_text_model()\n",
    "\n",
    "    source_image = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "    realclass_count=0\n",
    "    number_inputs=0\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    image_layerno_1,image_layerno_2= get_hidden_layer()\n",
    "  \n",
    "    #batch_size=config['batch_size'] \n",
    "    batch_size=4\n",
    "    batch_iterator = tqdm(validation_ds.iter(batch_size))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in batch_iterator :\n",
    "            count += 1\n",
    "\n",
    "########################  image KQV extraction \n",
    "            cropped_image=[]\n",
    "            allclass_numers=[]  \n",
    "    \n",
    "########################  image KQV extraction \n",
    "            num_image=0\n",
    "            images_list=batch['image']\n",
    "            #print(images_list)\n",
    "            allbbox=batch['objects']\n",
    "            #allclass_num=batch['objects']['category']\n",
    "            #print('numberofclass',allclass_num)\n",
    "########################  image KQV extraction \n",
    "            for bb in range(0,batch_size ):    \n",
    "                num_objects=len(allbbox[bb]['bbox'])\n",
    "                #allclass_numers.append(batch['objects'][bb]['category'])\n",
    "                #print('numberofclass',allclass_numers)\n",
    "                #print(\"numberof object at image \",num_objects)\n",
    "                images=images_list[bb].convert(\"RGB\")\n",
    "                for a in range(0,num_objects) :\n",
    "                  #  print(allbbox[bb]['bbox'][a])\n",
    "                    cropped_image.append(images.crop(allbbox[bb]['bbox'][a]) ) \n",
    "                    allclass_numers.append(batch['objects'][bb]['category'][a])\n",
    "                    num_image=num_image+1\n",
    "\n",
    "            image_1 = image_processor(images=cropped_image, return_tensors=\"pt\").to(device)\n",
    "            image_model=image_model.to(device)\n",
    "            image_outputs_1 = image_model(**image_1)\n",
    "            image_pooled_output_1 = image_outputs_1.pooler_output  \n",
    "            image_layear_output_1=image_outputs_1.hidden_states[image_layerno_1],image_outputs_1.hidden_states[image_layerno_2]\n",
    "\n",
    "\n",
    "                  \n",
    "\n",
    "            coco_classes = cocolabel()\n",
    "            rlabel = [None] * num_image\n",
    "            for x in range(num_image) :\n",
    "                rlabel[x]=coco_classes[allclass_numers[x]]\n",
    "            \n",
    "\n",
    "           # cifar10_classes = ['airplane']\n",
    "            \n",
    "          #  rlabel = [None] * batch_size\n",
    "          \n",
    "         #   for x in range(batch_size) :\n",
    "          #      rlabel[x]=cifar10_classes[label[x]]\n",
    "\n",
    "\n",
    "            inputs_text = text_Tokenizer(text=coco_classes, padding=True, return_tensors=\"pt\").to(device)\n",
    "            text_model= text_model.to(device)\n",
    "            text_outputs = text_model(**inputs_text)\n",
    "            text_pooled_output = text_outputs.pooler_output\n",
    "           # print(\"text eddddddddddddddddddddddd\",text_pooled_output.shape )\n",
    "            #text_layear_output=text_outputs.hidden_states[image_layerno_1],text_outputs.hidden_states[image_layerno_2]\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "  \n",
    "#############################################################################\n",
    "\n",
    "\n",
    "            #Text model parameters\n",
    "\n",
    "  #          text_query_bias=text_model.encoder.layer[10].attention.self.query.bias,text_model.encoder.layer[9].attention.self.query.bias \n",
    "            \n",
    " #           text_key_bias=text_model.encoder.layer[10].attention.self.key.bias,text_model.encoder.layer[9].attention.self.key.bias\n",
    " ###           text_value_bias=text_model.encoder.layer[10].attention.self.value.bias,text_model.encoder.layer[9].attention.self.value.bias\n",
    " #           _start = 0\n",
    "  #          _end = config[\"d_model\"]\n",
    "  #          text_query_weight=text_model.encoder.layer[10].attention.self.query.weight[_start:_end, :],text_model.encoder.layer[9].attention.self.query.weight[_start:_end, :]\n",
    "#            text_key_weight=text_model.encoder.layer[10].attention.self.key.weight[_start:_end, :],text_model.encoder.layer[9].attention.self.key.weight[_start:_end, :]\n",
    " #           text_value_weight=text_model.encoder.layer[10].attention.self.value.weight[_start:_end, :],text_model.encoder.layer[9].attention.self.value.weight[_start:_end, :]\n",
    "\n",
    "\n",
    "            text_query_bias=text_model.encoder.layer[image_layerno_1].attention.attn.q.bias,text_model.encoder.layer[image_layerno_2].attention.attn.q.bias \n",
    "            text_key_bias=text_model.encoder.layer[image_layerno_1].attention.attn.k.bias,text_model.encoder.layer[image_layerno_2].attention.attn.k.bias\n",
    "            text_value_bias=text_model.encoder.layer[image_layerno_1].attention.attn.v.bias,text_model.encoder.layer[image_layerno_2].attention.attn.v.bias\n",
    "            _start = 0\n",
    "            _end = config[\"d_model\"]\n",
    "            text_query_weight=text_model.encoder.layer[image_layerno_1].attention.attn.q.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.q.weight[_start:_end, :]\n",
    "            text_key_weight=text_model.encoder.layer[image_layerno_1].attention.attn.k.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.k.weight[_start:_end, :]\n",
    "            text_value_weight=text_model.encoder.layer[image_layerno_1].attention.attn.v.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.v.weight[_start:_end, :]\n",
    "\n",
    "\n",
    "\n",
    "            #image model parameters \n",
    "            image_query_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.bias\n",
    "            image_key_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.bias\n",
    "            image_value_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.bias\n",
    "\n",
    "            image_query_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.weight[_start:_end, :]\n",
    "            image_key_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.weight[_start:_end, :]\n",
    "            image_value_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.weight[_start:_end, :]\n",
    "\n",
    "            #proj_output=model(config['d_model'], config['ffn_hidden'], config['num_heads'], config['drop_prob'], config['num_layers'], config['kn_vocab_size'], model_audio, image_model, outputs_audio, image_outputs)\n",
    "            appfor= []\n",
    "            scores_1 = [None] * 80\n",
    "            scores_2 = [None] * 80\n",
    "            scores_3 = [None] * 80\n",
    "            scores_4 = [None] * 80\n",
    "            \n",
    "            for img in range(0,num_image):\n",
    "              image_1 = image_processor(images=cropped_image[img], return_tensors=\"pt\").to(device)\n",
    "              image_model=image_model.to(device)\n",
    "              image_outputs_1 = image_model(**image_1)\n",
    "              image_pooled_output_1 = image_outputs_1.pooler_output  \n",
    "              image_layear_output_1=image_outputs_1.hidden_states[image_layerno_1],image_outputs_1.hidden_states[image_layerno_2]\n",
    "              \"\"\"\n",
    "              image_2 = image_processor(images=cropped_image[img+1], return_tensors=\"pt\").to(device)\n",
    "              image_model=image_model.to(device)\n",
    "              image_outputs_2 = image_model(**image_2)\n",
    "              image_pooled_output_2 = image_outputs_2.pooler_output  \n",
    "              image_layear_output_2=image_outputs_2.hidden_states[image_layerno_1],image_outputs_2.hidden_states[image_layerno_2]\n",
    "              \n",
    "              \n",
    "              image_3 = image_processor(images=cropped_image[img+2], return_tensors=\"pt\").to(device)\n",
    "              image_model=image_model.to(device)\n",
    "              image_outputs_3 = image_model(**image_3)\n",
    "              image_pooled_output_3 = image_outputs_3.pooler_output  \n",
    "              image_layear_output_3=image_outputs_3.hidden_states[image_layerno_1],image_outputs_3.hidden_states[image_layerno_2]\n",
    "              \n",
    "              image_4 = image_processor(images=cropped_image[img+3], return_tensors=\"pt\").to(device)\n",
    "              image_model=image_model.to(device)\n",
    "              image_outputs_4 = image_model(**image_4)\n",
    "              image_pooled_output_4 = image_outputs_4.pooler_output  \n",
    "              image_layear_output_4=image_outputs_4.hidden_states[image_layerno_1],image_outputs_4.hidden_states[image_layerno_2]\n",
    "              \"\"\"\n",
    "              \n",
    "              #image model parameters \n",
    "              image_query_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.bias\n",
    "              image_key_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.bias\n",
    "              image_value_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.bias\n",
    "\n",
    "              image_query_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.weight[_start:_end, :]\n",
    "              image_key_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.weight[_start:_end, :]\n",
    "              image_value_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.weight[_start:_end, :]\n",
    "\n",
    "              \n",
    "              scores = [None] * 80\n",
    "              for x in range(0, 80):\n",
    "  \n",
    "                inputs_text = text_Tokenizer(text=coco_classes[x], padding=True, return_tensors=\"pt\").to(device)\n",
    "                text_model= text_model.to(device)\n",
    "                text_outputs = text_model(**inputs_text)\n",
    "                text_pooled_output = text_outputs.pooler_output\n",
    "                \n",
    "                \n",
    "                \n",
    "                text_query_bias=text_model.encoder.layer[image_layerno_1].attention.attn.q.bias,text_model.encoder.layer[image_layerno_2].attention.attn.q.bias \n",
    "                text_key_bias=text_model.encoder.layer[image_layerno_1].attention.attn.k.bias,text_model.encoder.layer[image_layerno_2].attention.attn.k.bias\n",
    "                text_value_bias=text_model.encoder.layer[image_layerno_1].attention.attn.v.bias,text_model.encoder.layer[image_layerno_2].attention.attn.v.bias\n",
    "                _start = 0\n",
    "                _end = config[\"d_model\"]\n",
    "                text_query_weight=text_model.encoder.layer[image_layerno_1].attention.attn.q.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.q.weight[_start:_end, :]\n",
    "                text_key_weight=text_model.encoder.layer[image_layerno_1].attention.attn.k.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.k.weight[_start:_end, :]\n",
    "                text_value_weight=text_model.encoder.layer[image_layerno_1].attention.attn.v.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.v.weight[_start:_end, :]\n",
    "\n",
    "\n",
    "                text_layear_output=text_outputs.hidden_states[image_layerno_1],text_outputs.hidden_states[image_layerno_2]\n",
    "                text_pooled_output =text_pooled_output.detach().cpu().numpy()\n",
    "                  \n",
    "                batch_size_new=1  \n",
    "                proj_output_1=model(batch_size_new,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias, image_key_bias,image_value_bias,\n",
    "                                      image_query_weight,image_key_weight,image_value_weight, text_layear_output, image_layear_output_1, image_pooled_output_1).to(device)\n",
    "                proj_output_1 = proj_output_1.detach().cpu().numpy()\n",
    "                \"\"\"\n",
    "                proj_output_2=model(batch_size_new,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias, image_key_bias,image_value_bias,\n",
    "                                    image_query_weight,image_key_weight,image_value_weight, text_layear_output, image_layear_output_2, image_pooled_output_2).to(device)  \n",
    "                      \n",
    "                proj_output_2 = proj_output_2.detach().cpu().numpy()\n",
    "\n",
    "              \n",
    "                \n",
    "                proj_output_3=model(batch_size_new,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias, image_key_bias,image_value_bias,\n",
    "                                    image_query_weight,image_key_weight,image_value_weight, text_layear_output, image_layear_output_3, image_pooled_output_3).to(device)\n",
    "                    \n",
    "                proj_output_3 = proj_output_3.detach().cpu().numpy()\n",
    "   \n",
    "                  \n",
    "\n",
    "                    \n",
    "                proj_output_4=model(batch_size_new,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias, image_key_bias,image_value_bias,\n",
    "                                    image_query_weight,image_key_weight,image_value_weight, text_layear_output, image_layear_output_4, image_pooled_output_4).to(device)  \n",
    "                          \n",
    "                proj_output_4 = proj_output_4.detach().cpu().numpy()   \n",
    "                    \n",
    "\n",
    "                \"\"\"\n",
    "\n",
    " \n",
    "                scores_1[x] = np.dot(proj_output_1, text_pooled_output.T)/(norm(proj_output_1)*(norm(text_pooled_output))) \n",
    "                #scores_2[x] = np.dot(proj_output_2, text_pooled_output.T)/(norm(proj_output_2)*(norm(text_pooled_output))) \n",
    "                #scores_3[x] = np.dot(proj_output_3, text_pooled_output.T)/(norm(proj_output_3)*(norm(text_pooled_output))) \n",
    "                #scores_4[x] = np.dot(proj_output_4, text_pooled_output.T)/(norm(proj_output_4)*(norm(text_pooled_output))) \n",
    "\n",
    "            \n",
    "              real_class=np.argmax(scores_1)\n",
    "           \n",
    "             # print(allclass_numers[img],real_class)\n",
    "               \n",
    "              #for a in range(0, 4): \n",
    "              #  print(allclass_numers[img+a],real_class)\n",
    "               # if allclass_numers[img+a] == real_class[a]:\n",
    "               #    realclass_count=realclass_count+1\n",
    "         \n",
    "                #  print(\"wwwwwwwwwww\",number_inputs)\n",
    "              #number_inputs=number_inputs+4\n",
    "               \n",
    "              #print(allclass_numers[img],real_class)\n",
    "              if allclass_numers[img] == real_class:\n",
    "                  realclass_count=realclass_count+1\n",
    "         \n",
    "               #   print(\"wwwwwwwwwww\")\n",
    "              number_inputs=number_inputs+1\n",
    "          \n",
    "            #print(\"fsfdsfds\",realclass_count,number_inputs)\n",
    "               \n",
    "            #print(\"cccccccccccccccccccc\",np.argmax(scores))\n",
    "            #scores2=np.argmax(appfor, axis=1)\n",
    "\n",
    "            #print(\"scores eddddddddddddddddddddddd\",scores2)\n",
    "            #print(\"scores eddddddddddddddddddddddd\",np.argmax(scores, axis=1))\n",
    "           # print(\"compare two class\",label)\n",
    "\n",
    "      \n",
    "\n",
    "            \n",
    "\n",
    "            #source_image.append(scores)\n",
    "            expected.append(1.00)\n",
    "            predicted.append(scores)\n",
    "            print(\"Accuracy\",realclass_count,number_inputs)\n",
    "            \n",
    "\n",
    "   # true_preds = []\n",
    "   # for i, label in enumerate(validation_ds[\"label\"]):\n",
    "    #    if label == preds[i]:\n",
    "   #        true_preds.append(1)\n",
    "   #     else:\n",
    "     #      true_preds.append(0)\n",
    "\n",
    "    Accuracy =(realclass_count / number_inputs*100 )\n",
    "\n",
    "    print(\"Accuracy\", Accuracy,realclass_count,number_inputs, \"%\")\n",
    "    return Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/maithri/.cache/huggingface/datasets/detection-datasets___parquet/detection-datasets--coco-64ef6d5414f6b8df/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Found cached dataset parquet (/home/maithri/.cache/huggingface/datasets/detection-datasets___parquet/detection-datasets--coco-64ef6d5414f6b8df/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset : 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image layer number 10 9\n",
      "No model to preload, starting from scratch\n",
      "Layers  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 14it [01:27,  1.05s/it, loss=-0.757]"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model(config):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "   #device=\"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    train_dataset_both ,val_dataset_both ,train_dataloader,val_dataloader= get_ds()\n",
    "   # train_dataset_both2 =train_dataset_both.shard(num_shards=10, index=0)\n",
    "    print(\"Size of the dataset :\", len(train_dataset_both))\n",
    "    image_model,image_processor =get_image_model()\n",
    "    model_audio,processor_audio =get_audio_model()\n",
    "    text_model,text_Tokenizer =get_text_model()\n",
    "    image_layerno_1,image_layerno_2= get_hidden_layer()\n",
    "    \n",
    "\n",
    "    model = get_model().to(device)\n",
    "\n",
    "     # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "\n",
    "   # optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    value_opt = torch.optim.Adam(params = model.parameters(), lr = config['lr'])\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    #loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)   \n",
    "    \n",
    "\n",
    "    \n",
    "    #siprint(\"image layer number\",image_layerno_1)\n",
    "\n",
    "   # abc=torch.rand(1, 768).to(device)\n",
    "    print(\"Layers \",config[\"num_layers\"])\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        #print(\"epoch\",epoch)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        #train_dataset_both ,val_dataset_both ,train_dataloader,val_dataloader= get_ds()\n",
    "        model.train()\n",
    "        batch_size=config['batch_size'] \n",
    "        \n",
    "        batch_iterator = tqdm(train_dataset_both.iter(batch_size), desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        #print(\"batch_iterator,batch_iterator\",batch_iterator)\n",
    "        \n",
    "\n",
    "        \n",
    "        for batch in batch_iterator:\n",
    "            cropped_image=[]\n",
    "            allclass_numers=[]  \n",
    "    \n",
    "########################  image KQV extraction \n",
    "            num_image=0\n",
    "            images_list=batch['image']\n",
    "            #print(images_list)\n",
    "            allbbox=batch['objects']\n",
    "            #allclass_num=batch['objects']['category']\n",
    "            #print('numberofclass',allclass_num)\n",
    "########################  image KQV extraction \n",
    "            for bb in range(0,batch_size ):    \n",
    "                num_objects=len(allbbox[bb]['bbox'])\n",
    "                #allclass_numers.append(batch['objects'][bb]['category'])\n",
    "                #print('numberofclass',allclass_numers)\n",
    "                #print(\"numberof object at image \",num_objects)\n",
    "                images=images_list[bb].convert(\"RGB\")\n",
    "                for a in range(0,num_objects) :\n",
    "                  #  print(allbbox[bb]['bbox'][a])\n",
    "                    cropped_image.append(images.crop(allbbox[bb]['bbox'][a]) ) \n",
    "                    allclass_numers.append(batch['objects'][bb]['category'][a])\n",
    "                    num_image=num_image+1\n",
    "            \n",
    "            #print(\"batch_images\",num_image)         \n",
    "                \n",
    "            #print(\"images\",cropped_image)    \n",
    "                \n",
    "            #allclass_numers2=np.squeeze(allclass_numers)\n",
    "            #print('numberofclass',allclass_numers )\n",
    "            \n",
    "            image = image_processor(images=cropped_image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "            image_model=image_model.to(device)\n",
    "            image_outputs = image_model(**image)\n",
    "            image_last_hidden_state = image_outputs.last_hidden_state\n",
    "            image_pooled_output = image_outputs.pooler_output  \n",
    "           # batch_nummber+= 1\n",
    "         \n",
    "            image_layear_output=image_outputs.hidden_states[image_layerno_1],image_outputs.hidden_states[image_layerno_2]\n",
    "\n",
    "            \n",
    "\n",
    "            coco_classes = cocolabel()\n",
    "            rlabel = [None] * num_image\n",
    "            for x in range(num_image) :\n",
    "                rlabel[x]=coco_classes[allclass_numers[x]]\n",
    "\n",
    "           \n",
    "            #print(\"imageddddddddddddddddddddddd\",label,flabel)\n",
    "            inputs_text = text_Tokenizer(text=rlabel, padding=True, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            text_model= text_model.to(device)\n",
    "            text_outputs = text_model(**inputs_text)\n",
    "            text_pooled_output = text_outputs.pooler_output\n",
    "\n",
    "            text_layear_output=text_outputs.hidden_states[image_layerno_1],text_outputs.hidden_states[image_layerno_2]\n",
    "            \n",
    "            batch_size_new=len(allclass_numers)\n",
    "            #audio model parameters\n",
    "          #  text_query_bias=text_model.encoder.layer[10].attention.self.query.bias,text_model.encoder.layer[9].attention.self.query.bias \n",
    "         #   text_key_bias=text_model.encoder.layer[10].attention.self.key.bias,text_model.encoder.layer[9].attention.self.key.bias\n",
    "          #  text_value_bias=text_model.encoder.layer[10].attention.self.value.bias,text_model.encoder.layer[9].attention.self.value.bias\n",
    "          #  _start = 0\n",
    "          #  _end = config[\"d_model\"]\n",
    "           # text_query_weight=text_model.encoder.layer[10].attention.self.query.weight[_start:_end, :],text_model.encoder.layer[9].attention.self.query.weight[_start:_end, :]\n",
    "           # text_key_weight=text_model.encoder.layer[10].attention.self.key.weight[_start:_end, :],text_model.encoder.layer[9].attention.self.key.weight[_start:_end, :]\n",
    "           # text_value_weight=text_model.encoder.layer[10].attention.self.value.weight[_start:_end, :],text_model.encoder.layer[9].attention.self.value.weight[_start:_end, :]\n",
    "\n",
    "            text_query_bias=text_model.encoder.layer[image_layerno_1].attention.attn.q.bias,text_model.encoder.layer[image_layerno_2].attention.attn.q.bias \n",
    "            text_key_bias=text_model.encoder.layer[image_layerno_1].attention.attn.k.bias,text_model.encoder.layer[image_layerno_2].attention.attn.k.bias\n",
    "            text_value_bias=text_model.encoder.layer[image_layerno_1].attention.attn.v.bias,text_model.encoder.layer[image_layerno_2].attention.attn.v.bias\n",
    "            _start = 0\n",
    "            _end = config[\"d_model\"]\n",
    "            text_query_weight=text_model.encoder.layer[image_layerno_1].attention.attn.q.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.q.weight[_start:_end, :]\n",
    "            text_key_weight=text_model.encoder.layer[image_layerno_1].attention.attn.k.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.k.weight[_start:_end, :]\n",
    "            text_value_weight=text_model.encoder.layer[image_layerno_1].attention.attn.v.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.v.weight[_start:_end, :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #image model parameters \n",
    "            image_query_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.bias\n",
    "            image_key_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.bias\n",
    "            image_value_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.bias\n",
    "\n",
    "            image_query_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.weight[_start:_end, :]\n",
    "            image_key_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.weight[_start:_end, :]\n",
    "            image_value_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.weight[_start:_end, :]\n",
    "            \n",
    "            #proj_output=model(config['d_model'], config['ffn_hidden'], config['num_heads'], config['drop_prob'], config['num_layers'], config['kn_vocab_size'], model_audio, image_model, outputs_audio, image_outputs)\n",
    "            proj_output=model(batch_size_new,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias,image_key_bias,image_value_bias, image_query_weight,image_key_weight,image_value_weight, \n",
    "                              text_layear_output, image_layear_output,image_pooled_output).to(device)\n",
    "           # out_image = Transformer(audio_query_bias,audio_key_bias,audio_value_bias,audio_query_weight,audio_key_weight,audio_value_weight, image_model, audio_layear_output, image_layear_output)\n",
    "            text_pooled_output=text_pooled_output/text_pooled_output.norm(dim=-1, keepdim=True)\n",
    "\n",
    "      \n",
    "            #image_pooled_output=torch.squeeze(image_pooled_output,dim=0).to(device)\n",
    "  \n",
    "            proj_output=proj_output/proj_output.norm(dim=-1, keepdim=True)\n",
    "            image_last_hidden_state=image_last_hidden_state/image_last_hidden_state.norm(dim=-1, keepdim=True)\n",
    "            image_pooled_output=image_pooled_output/image_pooled_output.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            loss = loss_fn(proj_output.view(-1,768), text_pooled_output .view(-1,768))   # compare with pooled_output image to audio\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        \n",
    "            global_step += 1\n",
    "            \n",
    "\n",
    "\n",
    "       # print(\"Numer of errors \",clipvalidation2,clipvalidation ) \n",
    "        # Run validation at the end of every epoch\n",
    "        # Accuracy = run_validation(model, val_dataloader, device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "        #Accuracy = run_validation(model, val_dataset_both, device,  lambda msg: batch_iterator.write(msg), 0, None, num_examples=100)\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    config = get_config()\n",
    "    train_model(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import config\n",
    "# config the database  \n",
    "#def get_ds_for_validation():\n",
    "    \n",
    "\n",
    "   # dataset_val = load_dataset(\"csv\", data_files=\"test.csv\",split=\"train\")\n",
    "\n",
    "\n",
    "\n",
    "   # train_dataloader = DataLoader(train_dataset_both, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "  #  val_dataloader = DataLoader(dataset_val, batch_size=1, shuffle=True)\n",
    "   # val_dataloader = DataLoader(dataset_val, batch_size=1 )\n",
    "\n",
    "\n",
    "   # return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(\"Using device:\", device)\n",
    "#config = get_config()\n",
    "#train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "##train_dataset_both ,val_dataset_both ,train_dataloader, val_dataloader= get_ds()\n",
    "#val_dataloader= get_ds_for_validation()\n",
    "#model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "##model = get_model().to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "#model_filename = latest_weights_file_path(config)\n",
    "#model_filename =\"weights/tmodel_00.pt\"\n",
    "#print(model_filename)\n",
    "#state = torch.load(model_filename)\n",
    "#model.load_state_dict(state['model_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_validation(model, val_dataset_both , device,  lambda msg: print(msg), 0, None, num_examples=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "config = get_config()\n",
    "train_dataset_both ,val_dataset_both ,train_dataloader, val_dataloader= get_ds()\n",
    "val_dataset_both2 =val_dataset_both.shard(num_shards=50, index=0)\n",
    "model = get_model().to(device)\n",
    "Accuracy = []\n",
    "for number in range(0, 40, 1):\n",
    "    if number < 10: \n",
    "        str(number)\n",
    "        number= \"0\"+ str(number)\n",
    "    else:\n",
    "        number=str(number)\n",
    "    cwd2 = \"tmodel_\" + number + \".pt\"\n",
    "    cwd = \"weights\"\n",
    "    os.path.join(cwd,cwd2)\n",
    "\n",
    "\n",
    "\n",
    "    # Load the pretrained weights\n",
    "    model_filename = latest_weights_file_path(config)\n",
    "    model_filename =os.path.join(cwd,cwd2)\n",
    "    print(model_filename)\n",
    "    state = torch.load(model_filename)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    tAccuracy = run_validation(model, val_dataset_both2, device,  lambda msg: print(msg), 0, None, num_examples=2500)\n",
    "    Accuracy.append(tAccuracy)\n",
    "    print(\"mairgh\",tAccuracy)\n",
    "    with open('output.txt', 'a') as testwritefile:\n",
    "        testwritefile.write(str(number))\n",
    "        testwritefile.write(\"layer accuracy \")\n",
    "        testwritefile.write(str(tAccuracy ))\n",
    "        testwritefile.write(\"\\n \")\n",
    "print(Accuracy)\n",
    "max_value_index = np.argmax(Accuracy)\n",
    "Maximum_Accuracy = Accuracy[max_value_index]\n",
    "with open('output.txt', 'a') as testwritefile:\n",
    "        testwritefile.write(\"Maximum_Accuracy:\")\n",
    "        testwritefile.write(str(Maximum_Accuracy))\n",
    "        testwritefile.write(\"Maximum Accuracy epoch:\")\n",
    "        testwritefile.write(str(max_value_index))\n",
    "print(\"Maximum_Accuracy:\", Maximum_Accuracy ,\"Maximum Accuracy epoch:\",max_value_index )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
